{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPn2StBeM8hOGUqOYW/VKp6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Junkai03/kat/blob/main/KAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qHNSASrwm2c",
        "outputId": "4af17c95-b3e7-4557-9211-0175a84010ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Sun Nov 24 20:01:16 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries\n",
        "!pip install timm\n",
        "!pip install wandb\n",
        "\n",
        "# Clone the project code\n",
        "!git clone https://github.com/Adamdad/rational_kat_cu.git\n",
        "%cd rational_kat_cu\n",
        "!pip install -e .\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxWl2PT3uV8R",
        "outputId": "cbebecac-da87-4218-bb9b-537d448be67e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.11)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.26.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "fatal: destination path 'rational_kat_cu' already exists and is not an empty directory.\n",
            "/content/rational_kat_cu\n",
            "Obtaining file:///content/rational_kat_cu\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: kat_rational\n",
            "  Attempting uninstall: kat_rational\n",
            "    Found existing installation: kat_rational 0.3\n",
            "    Uninstalling kat_rational-0.3:\n",
            "      Successfully uninstalled kat_rational-0.3\n",
            "  Running setup.py develop for kat_rational\n",
            "Successfully installed kat_rational-0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "from kat_rational import KAT_Group\n",
        "print(\"KAT_Group imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBjfPlODx6c1",
        "outputId": "27128af8-745b-4a91-e3ab-c4bc69761660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KAT_Group imported successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/rational_kat_cu/kat_rational/kat_1dgroup.py:10: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
            "/content/rational_kat_cu/kat_rational/kat_1dgroup.py:32: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, grad_output):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dV7IeRYckczJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from katransformer import Block"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Patch Embedding for image input.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        # Conv2d for patch embedding\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: [Batch, Channels, Height, Width]\n",
        "        x = self.proj(x)  # [Batch, Embed Dim, Num Patches Height, Num Patches Width]\n",
        "        x = x.flatten(2).transpose(1, 2)  # [Batch, Num Patches, Embed Dim]\n",
        "        x = x + self.pos_embed  # Add positional embedding\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "lpg1MdQ0z3My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StackBlocks(nn.Module):\n",
        "    \"\"\"\n",
        "    Stack two Transformer Blocks for feature extraction.\n",
        "    \"\"\"\n",
        "    def __init__(self, block, embed_dim=768, num_heads=8, mlp_ratio=4., proj_drop=0.1,\n",
        "                 attn_drop=0.1, drop_path=0.1, act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        # Initialize two stacked Blocks\n",
        "        self.block1 = block(\n",
        "            dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            proj_drop=proj_drop,\n",
        "            attn_drop=attn_drop,\n",
        "            drop_path=drop_path,\n",
        "            act_layer=act_layer,\n",
        "            norm_layer=norm_layer,\n",
        "        )\n",
        "        self.block2 = block(\n",
        "            dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            proj_drop=proj_drop,\n",
        "            attn_drop=attn_drop,\n",
        "            drop_path=drop_path,\n",
        "            act_layer=act_layer,\n",
        "            norm_layer=norm_layer,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass input through the first Block\n",
        "        x1 = self.block1(x)\n",
        "        # Pass the output of the first Block to the second Block\n",
        "        x2 = self.block2(x1)\n",
        "        return x2\n"
      ],
      "metadata": {
        "id": "gGPg1GNMyrVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureFusion(nn.Module):\n",
        "    \"\"\"\n",
        "    Feature fusion module for combining outputs from different Blocks.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, fusion_type='concat', num_blocks=2):\n",
        "        super().__init__()\n",
        "        self.fusion_type = fusion_type\n",
        "        if fusion_type == 'concat':\n",
        "            self.fc = nn.Linear(embed_dim * num_blocks, embed_dim)  # Reduce concatenated features\n",
        "        elif fusion_type == 'weighted_sum':\n",
        "            self.weights = nn.Parameter(torch.ones(num_blocks))  # Learnable weights for each block\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            features: List of features from different Blocks, each with shape [Batch, Num Patches, Embed Dim].\n",
        "        Returns:\n",
        "            Fused features with shape [Batch, Num Patches, Embed Dim] or [Batch, Embed Dim].\n",
        "        \"\"\"\n",
        "        if self.fusion_type == 'concat':\n",
        "            # Concatenate features along the last dimension\n",
        "            fused = torch.cat(features, dim=-1)  # [Batch, Num Patches, Embed Dim * Num Blocks]\n",
        "            fused = self.fc(fused)  # [Batch, Num Patches, Embed Dim]\n",
        "        elif self.fusion_type == 'weighted_sum':\n",
        "            # Apply weights to each feature and sum them\n",
        "            weights = torch.softmax(self.weights, dim=0)  # Normalize weights\n",
        "            fused = sum(w * f for w, f in zip(weights, features))  # [Batch, Num Patches, Embed Dim]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported fusion_type: {self.fusion_type}\")\n",
        "        return fused\n"
      ],
      "metadata": {
        "id": "MDK-r_c50cF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StackBlocksWithFusion(nn.Module):\n",
        "    \"\"\"\n",
        "    Stack Transformer Blocks and fuse their features.\n",
        "    \"\"\"\n",
        "    def __init__(self, block, embed_dim=768, num_heads=8, mlp_ratio=4., proj_drop=0.1,\n",
        "                 attn_drop=0.1, drop_path=0.1, fusion_type='concat', act_layer=nn.GELU,\n",
        "                 norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        # Initialize two Blocks\n",
        "        self.block1 = block(\n",
        "            dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            proj_drop=proj_drop,\n",
        "            attn_drop=attn_drop,\n",
        "            drop_path=drop_path,\n",
        "            act_layer=act_layer,\n",
        "            norm_layer=norm_layer,\n",
        "        )\n",
        "        self.block2 = block(\n",
        "            dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            proj_drop=proj_drop,\n",
        "            attn_drop=attn_drop,\n",
        "            drop_path=drop_path,\n",
        "            act_layer=act_layer,\n",
        "            norm_layer=norm_layer,\n",
        "        )\n",
        "        # Feature fusion module\n",
        "        self.fusion = FeatureFusion(embed_dim, fusion_type=fusion_type, num_blocks=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through Block 1\n",
        "        feature1 = self.block1(x)\n",
        "        # Pass through Block 2\n",
        "        feature2 = self.block2(feature1)\n",
        "        # Fuse features\n",
        "        fused_features = self.fusion([feature1, feature2])\n",
        "        return fused_features\n"
      ],
      "metadata": {
        "id": "LLuTLhze0fyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KAN(nn.Module):\n",
        "    \"\"\"\n",
        "    Kolmogorov–Arnold Network (KAN) for classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.1):\n",
        "        super().__init__()\n",
        "        hidden_features = hidden_features or in_features\n",
        "        out_features = out_features or in_features\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.drop1 = nn.Dropout(drop)\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop2 = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "9Jq1hCDk1MdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MedicalImageClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Medical Image Classifier using Patch Embedding, Transformer Blocks, Feature Fusion, and KAN.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=2,\n",
        "                 embed_dim=768, num_heads=8, mlp_ratio=4., proj_drop=0.1,\n",
        "                 attn_drop=0.1, drop_path=0.1, fusion_type='concat', act_layer=nn.GELU,\n",
        "                 norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        # Patch Embedding\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)\n",
        "\n",
        "        # Transformer Blocks with Feature Fusion\n",
        "        self.stack_blocks = StackBlocksWithFusion(\n",
        "            block=Block,\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            proj_drop=proj_drop,\n",
        "            attn_drop=attn_drop,\n",
        "            drop_path=drop_path,\n",
        "            fusion_type=fusion_type,\n",
        "            act_layer=act_layer,\n",
        "            norm_layer=norm_layer,\n",
        "        )\n",
        "\n",
        "        # Classification Head (KAN)\n",
        "        self.classifier = KAN(in_features=embed_dim, hidden_features=embed_dim, out_features=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Patch Embedding\n",
        "        x = self.patch_embed(x)  # [Batch, Num Patches, Embed Dim]\n",
        "\n",
        "        # Transformer Blocks with Fusion\n",
        "        x = self.stack_blocks(x)  # [Batch, Num Patches, Embed Dim]\n",
        "\n",
        "        # Global Average Pooling (Reduce patches)\n",
        "        x = x.mean(dim=1)  # [Batch, Embed Dim]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(x)  # [Batch, Num Classes]\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "g4mPDnEK1RXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the StackBlocks class"
      ],
      "metadata": {
        "id": "AnJW0wOM0NDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Initialize the StackBlocks model\n",
        "    stack_model = StackBlocks(\n",
        "        block=Block,\n",
        "        embed_dim=768,\n",
        "        num_heads=8,\n",
        "        mlp_ratio=4.,\n",
        "        proj_drop=0.1,\n",
        "        attn_drop=0.1,\n",
        "        drop_path=0.1,\n",
        "        act_layer=nn.GELU,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "    )\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        stack_model = stack_model.cuda()\n",
        "\n",
        "    # Create a dummy input tensor\n",
        "    input_tensor = torch.randn(8, 196, 768)  # Batch size = 8, Patches = 196 (14x14), Embedding dim = 768\n",
        "\n",
        "    # Move input to GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        input_tensor = input_tensor.cuda()\n",
        "\n",
        "    # Forward pass\n",
        "    output = stack_model(input_tensor)\n",
        "    print(\"Output shape:\", output.shape)  # The output shape should be [8, 196, 768]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPWAxsn-yt98",
        "outputId": "e3bf0618-fbb9-43e7-cb0d-29653a3da161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([8, 196, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 将 Patch Embedding 与 StackBlocks 连接，完整测试数据流"
      ],
      "metadata": {
        "id": "ga0qMA840BE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Patch Embedding\n",
        "    patch_embed = PatchEmbedding(img_size=224, patch_size=16, in_chans=3, embed_dim=768)\n",
        "    if torch.cuda.is_available():\n",
        "        patch_embed = patch_embed.cuda()\n",
        "\n",
        "    # StackBlocks\n",
        "    stack_model = StackBlocks(\n",
        "        block=Block,\n",
        "        embed_dim=768,\n",
        "        num_heads=8,\n",
        "        mlp_ratio=4.,\n",
        "        proj_drop=0.1,\n",
        "        attn_drop=0.1,\n",
        "        drop_path=0.1,\n",
        "        act_layer=nn.GELU,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "    )\n",
        "    if torch.cuda.is_available():\n",
        "        stack_model = stack_model.cuda()\n",
        "\n",
        "    # Input: Dummy image tensor\n",
        "    input_image = torch.randn(8, 3, 224, 224)  # Batch of 8, RGB images, size 224x224\n",
        "    if torch.cuda.is_available():\n",
        "        input_image = input_image.cuda()\n",
        "\n",
        "    # Forward pass through Patch Embedding\n",
        "    patch_embeddings = patch_embed(input_image)  # [Batch, Num Patches, Embed Dim]\n",
        "    print(\"Patch Embedding shape:\", patch_embeddings.shape)\n",
        "\n",
        "    # Forward pass through StackBlocks\n",
        "    output = stack_model(patch_embeddings)\n",
        "    print(\"StackBlocks output shape:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PSJdOzez7ko",
        "outputId": "315091f7-2b9a-48e6-bc0b-65eb0c0f1173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patch Embedding shape: torch.Size([8, 196, 768])\n",
            "StackBlocks output shape: torch.Size([8, 196, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Feature Fusion Module"
      ],
      "metadata": {
        "id": "_83uzF5u0rYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Initialize Patch Embedding\n",
        "    patch_embed = PatchEmbedding(img_size=224, patch_size=16, in_chans=3, embed_dim=768)\n",
        "    if torch.cuda.is_available():\n",
        "        patch_embed = patch_embed.cuda()\n",
        "\n",
        "    # Initialize StackBlocks with Fusion\n",
        "    stack_model = StackBlocksWithFusion(\n",
        "        block=Block,\n",
        "        embed_dim=768,\n",
        "        num_heads=8,\n",
        "        mlp_ratio=4.,\n",
        "        proj_drop=0.1,\n",
        "        attn_drop=0.1,\n",
        "        drop_path=0.1,\n",
        "        fusion_type='concat',  # Options: 'concat' or 'weighted_sum'\n",
        "    )\n",
        "    if torch.cuda.is_available():\n",
        "        stack_model = stack_model.cuda()\n",
        "\n",
        "    # Dummy input image tensor\n",
        "    input_image = torch.randn(8, 3, 224, 224)  # Batch of 8, RGB images, size 224x224\n",
        "    if torch.cuda.is_available():\n",
        "        input_image = input_image.cuda()\n",
        "\n",
        "    # Forward pass through Patch Embedding\n",
        "    patch_embeddings = patch_embed(input_image)  # [Batch, Num Patches, Embed Dim]\n",
        "    print(\"Patch Embedding shape:\", patch_embeddings.shape)\n",
        "\n",
        "    # Forward pass through StackBlocks with Fusion\n",
        "    fused_features = stack_model(patch_embeddings)\n",
        "    print(\"Fused Features shape:\", fused_features.shape)  # Should be [Batch, Num Patches, Embed Dim]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMsiKdaG0j-B",
        "outputId": "aafe1b18-aca3-4cc1-ec4f-b8c86657c834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patch Embedding shape: torch.Size([8, 196, 768])\n",
            "Fused Features shape: torch.Size([8, 196, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the whole model"
      ],
      "metadata": {
        "id": "ll8f3qYe1Zrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Initialize the full model\n",
        "    model = MedicalImageClassifier(\n",
        "        img_size=224,\n",
        "        patch_size=16,\n",
        "        in_chans=3,\n",
        "        num_classes=2,  # Binary classification (e.g., diseased vs healthy)\n",
        "        embed_dim=768,\n",
        "        num_heads=8,\n",
        "        mlp_ratio=4.,\n",
        "        proj_drop=0.1,\n",
        "        attn_drop=0.1,\n",
        "        drop_path=0.1,\n",
        "        fusion_type='concat',  # Options: 'concat' or 'weighted_sum'\n",
        "    )\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "\n",
        "    # Dummy input image tensor\n",
        "    input_image = torch.randn(8, 3, 224, 224)  # Batch of 8, RGB images, size 224x224\n",
        "    if torch.cuda.is_available():\n",
        "        input_image = input_image.cuda()\n",
        "\n",
        "    # Forward pass\n",
        "    logits = model(input_image)\n",
        "    print(\"Logits shape:\", logits.shape)  # Output shape should be [8, Num Classes]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iRKtYlL1VVx",
        "outputId": "aa43178d-5d4e-4217-fa17-f1386d561f1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([8, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d2usniRM-xWS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}